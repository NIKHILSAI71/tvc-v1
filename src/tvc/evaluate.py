"""Model evaluation for Recurrent PPO TVC controller."""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Tuple

import jax
import jax.numpy as jnp
import numpy as np
from flax import serialization

from .curriculum import CurriculumStage, build_curriculum
from .dynamics import RocketParams
from .env import TvcEnv
from .policies import PolicyConfig, PolicyFunctions, build_policy_network
from .training import RunningNormalizer

LOGGER = logging.getLogger(__name__)


@dataclass
class EpisodeMetrics:
    """Metrics for a single evaluation episode."""
    success: bool
    episode_return: float
    final_position_error: float
    final_velocity_error: float
    final_orientation_alignment: float
    mean_position_error: float
    mean_velocity_error: float
    episode_length: int
    crashed: bool
    out_of_bounds: bool


@dataclass
class EvaluationResults:
    """Comprehensive evaluation results."""
    success_rate: float
    mean_return: float
    std_return: float
    mean_position_error: float
    mean_velocity_error: float
    mean_orientation_alignment: float
    crash_rate: float
    out_of_bounds_rate: float
    mean_episode_length: float
    episodes: List[EpisodeMetrics] = field(default_factory=list)


def load_policy(
    policy_path: Path,
    policy_config: PolicyConfig = PolicyConfig(),
) -> tuple[Any, PolicyFunctions, RunningNormalizer | None]:
    """Load trained recurrent policy and optional normalizer."""
    policy_funcs = build_policy_network(policy_config)

    with open(policy_path, "rb") as f:
        data = f.read()

    obs_rms = None
    try:
        loaded = serialization.from_bytes(None, data)
        
        if isinstance(loaded, dict) and "params" in loaded and "obs_rms" in loaded:
            params = loaded["params"]
            rms_dict = loaded["obs_rms"]
            if rms_dict:
                obs_rms = RunningNormalizer(
                    count=rms_dict.get("count", 0.0),
                    mean=jnp.array(rms_dict.get("mean", [])),
                    m2=jnp.array(rms_dict.get("m2", []))
                )
            LOGGER.info("âœ… Loaded NEW policy format (includes ObsRMS)")
        else:
            params = loaded
            LOGGER.info("âš ï¸ Loaded LEGACY policy format (No ObsRMS)")

    except Exception as e:
        LOGGER.warning("Error loading policy: %s", e)
        raise

    LOGGER.info("Loaded policy from %s", policy_path)
    return params, policy_funcs, obs_rms


def evaluate_policy(
    params: Any,
    policy_funcs: PolicyFunctions,
    env: TvcEnv,
    stage: CurriculumStage,
    obs_rms: RunningNormalizer | None = None,
    num_episodes: int = 100,
    deterministic: bool = True,
    seed: int = 42,
) -> EvaluationResults:
    """Evaluate recurrent policy performance."""
    LOGGER.info("=" * 80)
    LOGGER.info("ðŸŽ¯ EVALUATING RECURRENT POLICY: %s", stage.name)
    LOGGER.info("Episodes: %d | Deterministic: %s", num_episodes, deterministic)
    LOGGER.info("=" * 80)

    env.configure_stage(stage)
    
    # Initialize LSTM Carrier
    # We need a dummy init to get shapes, or use a helper
    config = env.model # Not config..
    # Hardcode dim or assume policy_config default
    lstm_dim = 128 # Default in policies.py provided earlier
    
    target_pos = np.array(stage.target_position, dtype=np.float32)
    target_vel = np.array(stage.target_velocity, dtype=np.float32)
    target_quat = np.array([1.0, 0.0, 0.0, 0.0], dtype=np.float32)

    episodes_data: List[EpisodeMetrics] = []

    for episode_idx in range(num_episodes):
        obs = env.reset()
        
        # Initialize Hidden State [Batch=1, Dim]
        # We need to use the policy's init logic or manually create zeros
        # Let's assume standard zero init
        hidden_h = jnp.zeros((1, lstm_dim))
        hidden_c = jnp.zeros((1, lstm_dim))
        hidden = (hidden_h, hidden_c)
        
        episode_return = 0.0
        done = False
        steps = 0

        position_errors = []
        velocity_errors = []
        orientation_alignments = []
        crashed = False
        out_of_bounds = False

        while not done and steps < env.max_steps:
             # Normalize observation if normalizer provided
            if obs_rms is not None:
                norm_obs = obs_rms.normalize(obs, update_stats=False)
            else:
                norm_obs = jnp.asarray(obs, dtype=jnp.float32)

            # Recurrent Step
            # Input: [1, Features]
            # Hidden: [1, Dim]
            # Dones: [1] (0.0)
            
            # policy_funcs.actor returns (mean_action, new_hidden)
            action, hidden = policy_funcs.actor(
                params, 
                norm_obs[None, :], 
                hidden, 
                jnp.array([0.0]), 
                key=None, 
                deterministic=deterministic
            )
            
            action_np = np.asarray(action[0], dtype=np.float32) # Squeeze batch

            result = env.step(action_np)
            episode_return += result.reward
            obs = result.observation
            done = result.done
            steps += 1

            pos = env.data.qpos[0:3]
            vel = env.data.qvel[0:3]
            quat = env.data.qpos[3:7]

            pos_error = float(np.linalg.norm(pos - target_pos))
            vel_error = float(np.linalg.norm(vel - target_vel))
            orient_alignment = float(np.abs(np.dot(quat, target_quat)))

            position_errors.append(pos_error)
            velocity_errors.append(vel_error)
            orientation_alignments.append(orient_alignment)

            if pos[2] < 0.2:
                crashed = True
            if np.linalg.norm(pos[0:2]) > 50.0 or pos[2] > 200.0:
                out_of_bounds = True

        final_pos_error = position_errors[-1] if position_errors else float('inf')
        final_vel_error = velocity_errors[-1] if velocity_errors else float('inf')
        final_orient_alignment = orientation_alignments[-1] if orientation_alignments else 0.0

        success = (
            final_pos_error < stage.position_tolerance and
            final_vel_error < stage.velocity_tolerance and
            final_orient_alignment > 0.95 and
            not crashed and
            not out_of_bounds
        )

        episodes_data.append(EpisodeMetrics(
            success=success,
            episode_return=episode_return,
            final_position_error=final_pos_error,
            final_velocity_error=final_vel_error,
            final_orientation_alignment=final_orient_alignment,
            mean_position_error=float(np.mean(position_errors)),
            mean_velocity_error=float(np.mean(velocity_errors)),
            episode_length=steps,
            crashed=crashed,
            out_of_bounds=out_of_bounds,
        ))

        if (episode_idx + 1) % 10 == 0:
            current_success_rate = sum(e.success for e in episodes_data) / len(episodes_data)
            LOGGER.info("Episode %3d/%d | Success: %.1f%% | Return: %7.2f",
                       episode_idx + 1, num_episodes, current_success_rate * 100, episode_return)

    # Aggregate
    successes = [e.success for e in episodes_data]
    returns = [e.episode_return for e in episodes_data]
    
    results = EvaluationResults(
        success_rate=float(np.mean(successes)),
        mean_return=float(np.mean(returns)),
        std_return=float(np.std(returns)),
        mean_position_error=float(np.mean([e.final_position_error for e in episodes_data])),
        mean_velocity_error=float(np.mean([e.final_velocity_error for e in episodes_data])),
        mean_orientation_alignment=float(np.mean([e.final_orientation_alignment for e in episodes_data])),
        crash_rate=float(np.mean([e.crashed for e in episodes_data])),
        out_of_bounds_rate=float(np.mean([e.out_of_bounds for e in episodes_data])),
        mean_episode_length=float(np.mean([e.episode_length for e in episodes_data])),
        episodes=episodes_data,
    )
    
    LOGGER.info("=" * 80)
    LOGGER.info("ðŸ“Š RESULTS: Success %.1f%% | Mean Return %.2f", results.success_rate * 100, results.mean_return)
    LOGGER.info("=" * 80)

    return results


def evaluate_across_curriculum(policy_path, output_dir=None, num_episodes=100, policy_config=PolicyConfig(), rocket_params=RocketParams(), seed=42):
    """Eval all stages."""
    params, policy_funcs, obs_rms = load_policy(policy_path, policy_config)
    env = TvcEnv(dt=0.02, ctrl_limit=0.14, max_steps=2000, seed=seed)
    env.apply_rocket_params(rocket_params)
    
    if obs_rms is None:
        obs_rms = RunningNormalizer.initialise(env.reset())
    
    curriculum = build_curriculum()
    stage_results = {}
    
    for stage in curriculum:
        res = evaluate_policy(params, policy_funcs, env, stage, obs_rms, num_episodes, seed=seed)
        stage_results[stage.name] = res
        
    if output_dir:
        # Saving logic handled similar to before (omitted for brevity but would exist)
        pass
        
    return stage_results
